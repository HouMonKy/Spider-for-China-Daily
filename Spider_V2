import re
import time
import requests
import pandas as pd
from lxml import etree
from selenium import webdriver
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup

# 正则表达式捕获对应文本
findTitle = re.compile(r'<h1\s*.*?>(.*?)</h1>')  # 捕获报道标题
findDate = re.compile(r'Updated:\s*(\d{4}-\d{2}-\d{2})\s*\d{2}:\d{2}\s*')  # 捕获报道日期
findSource = re.compile(r'\|?\s*([^|<>]+)\s*\|\s*Updated')  # 捕获来源


def main():
    # 输入源网站
    base_url = [
        "https://www.chinadaily.com.cn/world/special_coverage/647d4224a31033ad3f7ba6e9",
        "https://newssearch.chinadaily.com.cn/en/search?cond=%7B%22publishedDateFrom%22%3A%222013-09-01%22%2C%22publishedDateTo%22%3A%222023-11-24%22%2C%22titleMust%22%3A%2221st+century+maritime+silk+road%22%2C%22sort%22%3A%22dp%22%2C%22duplication%22%3A%22on%22%7D&language=en",
        "https://newssearch.chinadaily.com.cn/en/search?cond=%7B%22publishedDateFrom%22%3A%222013-09-01%22%2C%22publishedDateTo%22%3A%222023-11-24%22%2C%22titleMust%22%3A%22silk+road+economic+belt%22%2C%22sort%22%3A%22dp%22%2C%22duplication%22%3A%22on%22%7D&language=en",
        "https://newssearch.chinadaily.com.cn/en/search?cond=%7B%22publishedDateFrom%22%3A%222013-09-01%22%2C%22publishedDateTo%22%3A%222023-11-24%22%2C%22titleMust%22%3A%22BRI%22%2C%22sort%22%3A%22dp%22%2C%22duplication%22%3A%22on%22%7D&language=en",
        "https://newssearch.chinadaily.com.cn/en/search?cond=%7B%22publishedDateFrom%22%3A%222013-09-01%22%2C%22publishedDateTo%22%3A%222023-11-24%22%2C%22titleMust%22%3A%22belt+and+road%22%2C%22sort%22%3A%22dp%22%2C%22duplication%22%3A%22on%22%7D&language=en#"
    ]
    # 存储数据的路径
    save_path = r"C:\Users\29791\Desktop\Content.xlsx"
    get_data(base_url, save_path)


# 爬取全文并存储
def get_data(base_url, save_path):
    all_urls = scrape_all_urls(base_url)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
    }

    df = pd.DataFrame(columns=["Title", "Source", "Date", "Content", "ContentLink", "Note"])

    for i, one_url in enumerate(all_urls):
        try:
            res = requests.get(url=one_url, headers=headers)
            html = etree.HTML(res.text)
            html_source_code = etree.tostring(html, pretty_print=True, method='html').decode('utf-8')

            # 匹配标题并存储
            title_match = re.search(findTitle, html_source_code)
            title = title_match.group(1) if title_match else one_url

            # 匹配来源并储存
            source_match = re.search(findSource, html_source_code)
            source = source_match.group(1) if source_match else "No source found"

            # 匹配日期并存储
            date_match = re.search(findDate, html_source_code)
            date = date_match.group(1) if date_match else "No date found"

            # 匹配正文，并将正文txt链接存储
            paragraphs = html.xpath('//div[@id="Content"]//p/text()')
            content = "\n".join(paragraph.strip() for paragraph in paragraphs if paragraph.strip())

            # 判断是否有全文，有为T，无为F
            if html.xpath('//div[@id="Content"]//p//a'):
                note = 'F'
            else:
                note = 'T'

            print(title, source, date, content, one_url, note)
            df.loc[len(df.index)] = [title, source, date, content, one_url, note]

            # 计数器
            k = i + 1
            print(f'第{k}篇，还剩{len(all_urls)-k}篇')

        except Exception as e:
            print(f"Error processing URL {one_url}: {e}")

        df.to_excel(save_path, index=False)
        print("进程结束！")


def scrape_all_urls(base_url):
    all_urls = []
    count_urls = []  # 计数用
    for i in range(0, len(base_url)):
        # 模拟浏览器头部信息，用户代理
        options = webdriver.ChromeOptions()
        options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        chrome_exe_path = r"C:\Users\29791\Desktop\chrome-win64\chrome.exe"
        options.binary_location = chrome_exe_path
        driver = webdriver.Chrome(options=options)
        url = base_url[i]
        driver.get(url)
        time.sleep(7)  # 等待页面加载
        i = 0  # 初始化 i 的值

        while True:
            # 单个网站计数
            i += 1
            print(f'第{i}页')
            # 爬取网址
            try:
                # 使用 BeautifulSoup 解析 HTML 内容
                soup = BeautifulSoup(driver.page_source, "html.parser")
                # 普通网页
                href_pattern = None
                art_detail_tags = []
                # 获取所有含有链接的标签
                if soup.find_all('div', class_="art_detail"):
                    art_detail_tags = soup.find_all('div', class_="art_detail")
                    # 定义用于匹配 href 属性的正则表达式
                    href_pattern = re.compile(r'href="(https?://[^"]*chinadaily\.com\.cn[^"]*)"')
                # 特殊网页
                elif soup.find('div', class_="mb10 tw3_01_2"):
                    art_detail_tags = soup.find_all('div', class_="mb10 tw3_01_2")
                    # 定义用于匹配 href 属性的正则表达式
                    href_pattern = re.compile(r'href="//([^"]*chinadaily.com.cn[^"]*)"')
                # 在文本中匹配所有 href 属性对应的网址
                url_array = []
                for tag in art_detail_tags:
                    # 从标签中提取字符串
                    tag_str = str(tag)
                    # 在提取的字符串中匹配 href 属性对应的网址
                    matches = re.findall(href_pattern, tag_str)
                    if matches:
                        for match in matches:
                            if match.startswith(("http://", "https://")):
                                actual_link = match
                            else:
                                actual_link = "https://" + match
                            # 将匹配结果存储到数组中
                            url_array.append(actual_link)

                # 去除重复元素
                unique_urls = list(set(url_array))
                # 输出去重后的列表
                print(unique_urls)
                # 计数
                count_urls.extend(unique_urls)
                # 将爬取的网址添加到总列表中，此时列表中可能有重复网址
                all_urls.extend(unique_urls)

                # 翻页操作
                # 普通网页
                # 找到所有 <a> 标签
                if driver.find_elements(By.XPATH, '//div[@class="page rt"]//a'):
                    page_buttons = driver.find_elements(By.XPATH, '//div[@class="page rt"]//a')
                    # 分析当前结构，有的网站首页、尾页共10个翻页按钮，中间部分12个，有的则是其他情况
                    # 页面大于5时
                    if len(page_buttons) == 10:
                        # 判断第5个按钮是否为翻页按钮
                        if page_buttons[4].get_attribute('title') == 'next':
                            next_button = page_buttons[4]
                            # 翻页
                            next_button.click()
                            # 等待页面加载
                            time.sleep(7)
                        else:
                            # 此时是尾页
                            print(f'爬取完毕！本链接爬取{len(count_urls)}篇文稿。')
                            count_urls = []
                            break
                    # 页面小于5时
                    elif len(page_buttons) == 4:
                        # 判断第2个按钮是否为翻页按钮
                        if page_buttons[1].get_attribute('title') == 'next':
                            next_button = page_buttons[1]
                            # 翻页
                            next_button.click()
                            # 等待页面加载
                            time.sleep(7)
                        else:
                            # 此时是尾页
                            print(f'爬取完毕！本链接爬取{len(count_urls)}篇文稿。')
                            count_urls = []
                            break
                    else:
                        # 判断第6个按钮是否为翻页按钮
                        next_button = page_buttons[5]
                        # 翻页
                        next_button.click()
                        # 等待页面加载
                        time.sleep(7)

                # 特殊页面
                if driver.find_elements(By.XPATH, '//div[@id="div_currpage"]//a'):
                    page_buttons = driver.find_elements(By.XPATH, '//div[@id="div_currpage"]//a')
                    if len(page_buttons) == 11:
                        # 第10个按钮是翻页按钮
                        next_button = page_buttons[9]
                        # 翻页
                        next_button.click()
                        # 等待页面加载
                        time.sleep(7)
                    elif len(page_buttons) == 12:
                        # 第10个按钮是翻页按钮
                        next_button = page_buttons[10]
                        # 翻页
                        next_button.click()
                        # 等待页面加载
                        time.sleep(7)
                    elif len(page_buttons) == 13:
                        # 第10个按钮是翻页按钮
                        next_button = page_buttons[11]
                        # 翻页
                        next_button.click()
                        # 等待页面加载
                        time.sleep(7)
                    else:
                        # 此时是尾页
                        print(f'爬取完毕！本链接爬取{len(count_urls)}篇文稿。')
                        count_urls = []
                        break

            except Exception as e:
                print(f'爬取失败: {e}')

        # 关闭浏览器
        driver.quit()
        # 总列表去重
        all_urls = list(set(all_urls))
        print(f'一共爬取{len(all_urls)}篇文稿')
    return all_urls


if __name__ == "__main__":
    # 调用函数
    main()
